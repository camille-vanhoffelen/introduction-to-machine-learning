{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "â•°á•¦â•¯(à¹‘â€¢ .Ì« â€¢à¹‘)â•°á•¤â•¯\n",
    "\n",
    "We continue our pokemon trainer adventure with new best friends: non-linear supervised learning algorithms. You teach me, and I'll teach some models. There's no better team!\n",
    "\n",
    "The data can be found under `pokedex/pokemons.csv`, and is the same as assignment 1, 2, & 3. Run the cell below to get an overview of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('pokedex/pokemons.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "The rainbow bird we spotted in the sky last assignment was Ho-oh, a powerful pokemon who's feathers glow in seven colors ðŸŒˆ. Inspired by this encounter, we want to improve our legendary pokemon detector. Maybe we'll catch one next time! We know that Support Vector Machines (SVM) can learn non-linear decision boundaries, which could help make better predictions.\n",
    "\n",
    "ðŸ’ª **Task: Train a SVM classifier with Radial Basis Function (RBF) kernel which predicts if pokemons are `Legendary`.**\n",
    "- use `HP`, `Attack`, `Defense`, `Sp. Atk`, `Sp. Def`, and `Speed` as features\n",
    "- use `Legendary` as label\n",
    "- scale the features using standardization before you train the model\n",
    "- store your trained model in a variable called `svm`\n",
    "- use the `random_state`, `C`, and `gamma` hyperparameters provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 250\n",
    "C = 20\n",
    "gamma = 'scale'\n",
    "\n",
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def test_svm():\n",
    "    assert svm, \"Can't find svm, have you used the correct variable name for your model?\"\n",
    "    assert math.isclose(0.984, svm.score(X_scaled, y), rel_tol=1e-3), f\"Your model predictions don't look quite right\"\n",
    "    print('Success! ðŸŽ‰')\n",
    "    print('Ho-oh was classified as not legendary ðŸ˜ž')\n",
    "    \n",
    "test_svm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  **Task: List and describe the main steps that happen during your SVM model's _training_ , i.e inside of sklearn's `.fit()` method.**\n",
    "\n",
    "ðŸ§  **Bonus Task: Explain the effect of hyperparameters `C` and `gamma` on the SVM's optimization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Our SVM failed to recognise that the flamboyant Ho-oh is a legendary pokemon... Fortunately, we know other non-linear classifiers. Random forests are known for their flexibility, so they might fare better here.\n",
    "\n",
    "ðŸ’ª **Task: Train a Random Forest classifier which predicts if pokemons are `Legendary`.**\n",
    "- use `HP`, `Attack`, `Defense`, `Sp. Atk`, `Sp. Def`, and `Speed` as features\n",
    "- use `Legendary` as label\n",
    "- scale the features using standardization before you train the model\n",
    "- store your trained model in a variable called `random_forest`\n",
    "- use the `random_state`, `n_estimators`, and `min_samples_leaf` hyperparameters provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 250\n",
    "n_estimators = 100\n",
    "min_samples_leaf = 8\n",
    "\n",
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_forest():\n",
    "    assert random_forest, \"Can't find random_forest, have you used the correct variable name for your model?\"\n",
    "    assert math.isclose(0.954, random_forest.score(X_scaled, y), rel_tol=1e-3), f\"Your model predictions don't look quite right\"\n",
    "    print('Success! ðŸŽ‰')\n",
    "    print('Ho-oh was correctly classified as legendary ðŸ˜Ž')\n",
    "    \n",
    "test_random_forest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  **Task: List and describe the main steps that happen during your random forest model's _training_ , i.e inside of sklearn's `.fit()` method.**\n",
    "\n",
    "ðŸ§  **Bonus Task: Why do random forests generalize better than decision trees?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "The random forest recognized that Ho-oh is an absolute legend. But we heard that the A.I hype nowadays is all about neural networks... They are notoriously difficult to train, but we are the best trainer there ever was!\n",
    "\n",
    "ðŸ’ª **Task: Train a Neural Network classifier which predicts if pokemons are `Legendary`.**\n",
    "- use `HP`, `Attack`, `Defense`, `Sp. Atk`, `Sp. Def`, and `Speed` as features\n",
    "- use `Legendary` as label\n",
    "- scale the features using standardization before you train the model\n",
    "- store your trained model in a variable called `nn`\n",
    "- create a network with 2 hidden layers of 6 neurons each, with ReLU activation\n",
    "- use the `random_seed`, `optimizer`, `batch_size` and `epochs` hyperparameters provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 250\n",
    "optimizer = 'adam'\n",
    "batch_size=16\n",
    "epochs=200\n",
    "\n",
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_network():\n",
    "    assert nn, \"Can't find nn, have you used the correct variable name for your model?\"\n",
    "    assert nn.evaluate(X_scaled, y, verbose=0) < 0.1, f\"Your model predictions don't look quite right\"\n",
    "    print('Success! ðŸŽ‰')\n",
    "    print('Ho-oh was correctly classified as legendary ðŸ˜Ž')\n",
    "\n",
    "test_neural_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  **Task: List and describe the main steps that happen during your neural network's _training_ , i.e inside of keras's `.fit()` method.**\n",
    "\n",
    "ðŸ§  **Task: List the three different flavours of gradient descent. Which one did we use here? Why is it the most popular?**\n",
    "\n",
    "\n",
    "ðŸ§  **Bonus Task: If we used a `linear` activation for our hidden layers, would this model be as powerful? why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "We've trained our neural network, and it seems to correctly predict Ho-oh's legendary status. ðŸ’¯ We want to dig deeper into its optimization process by looking at its loss curve.\n",
    "\n",
    "ðŸ’ª **Task: Visualize your neural network's loss curve.**\n",
    "- store the model's training history in a variable called `history`\n",
    "- plot its loss curve\n",
    "- the curve values should be correct, but you will be mostly graded on your data visualization practices\n",
    "- revisit data viz tips from lecture 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_curve():\n",
    "    assert history, \"Can't find `history`, have you save the model's history\"\n",
    "    loss = history.history['loss']\n",
    "    assert len(loss) == 200, f\"There are {len(loss)} loss values. There should be 200, one per epoch\"\n",
    "    assert loss[-1] < 0.1, f\"Your final loss: {loss[-1]} should be less than 0.1\"\n",
    "    print('Success! ðŸŽ‰')\n",
    "    \n",
    "    \n",
    "test_loss_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  **Task: What does this loss curve say about our neural network training. Why?**\n",
    "\n",
    "ðŸ§  **Bonus Task: The loss curve doesn't seem to be converging to a loss of 0. List scenarios that could explain to this behaviour, and describe why.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "We were lead by âœ¨adamâœ¨ in our quest to be a pokemon master, but Professor Oak told us that he might not the messiah. ðŸ™€\n",
    "\n",
    "ðŸ’ª **Task: Train Neural Network classifiers with different optimizers. Then compare them by plotting their loss curves.**\n",
    "- use `HP`, `Attack`, `Defense`, `Sp. Atk`, `Sp. Def`, and `Speed` as features\n",
    "- use `Legendary` as label\n",
    "- scale the features using standardization before you train the model\n",
    "- train one neural network per optimizer listed in `optimizers`\n",
    "- each network should have 2 hidden layers of 6 neurons each, with ReLU activation\n",
    "- use the `random_seed`, `batch_size` and `epochs` hyperparameters provided below\n",
    "- plot each model's loss curve on the same graph\n",
    "- revisit data viz tips from lecture 2.6\n",
    "- the unit test is the graph ðŸ™ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 250\n",
    "batch_size=16\n",
    "epochs=200\n",
    "optimizers = ['sgd', 'rmsprop', 'adam', 'adadelta', 'adagrad', 'adamax', 'nadam']\n",
    "\n",
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  **Bonus Task: According to your graph, which is the best optimizer? Explain how this optimizer works _(you might have to look it up in the lecture resources)_.**\n",
    "\n",
    "ðŸ§  **Bonus Task: The best optimizer on your graph might still not be the best optimizer choice for our neural network. Why?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2.5: Text and Image Data\n",
    "\n",
    "This lecture, we are going to focus on two new types of data: text, and images.  We'll learn the basic data exploration skills required to get started with the two most popular fields of Machine Learning, Natural Language Processing (NLP), and Computer Vision (CV).\n",
    "\n",
    "**Learning goals:**\n",
    "\n",
    "- string operations\n",
    "- basic regular expressions\n",
    "- tokenization\n",
    "- image manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internet contains almost all of our civilization's knowledge. Why don't we just explore that data and use it? In a twist of irony similar to that of the [library of Babel](https://en.wikipedia.org/wiki/The_Library_of_Babel), what makes this knowledge so difficult to access is the fact that there is so much information. Being able to summarise, visualize, and explore text data is difficult, because it cannot be numerically aggregated like tabular data (see lecture 2.3 & 2.4). Words are symbols, we need some solutions to get insights into the meaning encoded in their sequences.\n",
    "\n",
    "One of the simplest tricks is to study the distribution of words. \n",
    "\n",
    "Seinfeld is a heritage of the 90s, and we'd like to revisit the era. The `Seinfeld.csv` dataset contains the transcript of all the dialogues of the first episode. Let's take a peek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('seinfeld.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What an introduction! We can see that the lines are formatted as such: `CHARACTER,CONTENT`. However, we wish to explore only the content. Let's cut off the character field by splitting the lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_character(string):\n",
    "    return ','.join(string.split(',')[1:])\n",
    "\n",
    "dialogue = [remove_character(l) for l in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† The code above uses a [list comprehension](https://realpython.com/list-comprehension-python/). Can you step through what's happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The character is gone, but the lines end in a newline. Let's remove those as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = [l.strip('\\n') for l in dialogue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that _some_ of the lines start/end with a `\"`. `.strip()` has the useful property of only executing if the character to remove is present. Let's use it for the `\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = [l.strip('\"') for l in dialogue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue[490]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines are getting cleaner, but now we have to deal with numbers and punctuation. We're interested in _word_ distributions, `32` doesn't tell us anything about Seinfeld! Let's filter numbers and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def filter_numbers(string):\n",
    "    return re.sub(r'\\d+', '', string)\n",
    "\n",
    "dialogue = [filter_numbers(l) for l in dialogue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue[490]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we used a regular expression, a.k.a [regex](https://regexr.com/). These are strings that define _search patterns_ in other strings. They are very useful when processing text data, but can get diabolically complicated! Thankfully, the one we need for filtering punctuation is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_punct(string):\n",
    "    return re.sub(r'[^\\w\\s]', '', string)\n",
    "\n",
    "dialogue = [filter_punct(l) for l in dialogue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue[492]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue[490]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we've ended up with some empty lines... Let's clean those up too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = [l for l in dialogue if l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue[490]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's lowercase our lines to normalise all word strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = [l.lower() for l in dialogue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue[490]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're close to seeing word distributions! But first, we must decide what is a \"word\"... Depending on how clean the text is, there are many ways of splitting a string into a sequence of word units. In the field of NLP, these units are called \"tokens\", and the process is refered to as \"tokenization\". Let's tokenize our lines by whitespace, meaning we'll split the string wherever there is at least one whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_whitespace(dialogue):\n",
    "    tokens = []\n",
    "    for l in dialogue:\n",
    "        tokens.extend(l.split())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokens = tokenize_whitespace(dialogue)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have words! Now we can look at the ten most common tokens using a [Counter](https://pymotw.com/2/collections/counter.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(tokens).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòê... All the most popular words are common words, like pronouns or determiners. This isn't really giving us insights into the show, or capturing the spirit of Seinfeld! We need more advanced text processing. One common solution is to remove [stopwords](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html) from our tokens. But for this, we'll need to use a more powerful NLP library.\n",
    "\n",
    "[Spacy](https://spacy.io/) is distinguishing itself as the leading NLP library in python. Let's try it out! It uses powerful ML models to do some of its text analysis, so we need to download those first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to analyse! Spacy's philosophy is to condense all the text processing one might wish for in one method called `.nlp()` (more details in their [tutorial](https://spacy.io/usage/spacy-101))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open('seinfeld.csv', 'r') as f:\n",
    "    content = f.read()\n",
    "doc = nlp(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we don't need to execute any of our manual processing, since `spacy` takes care of everything. All the text metadata is now available through the `doc` object. We can explore the word distribution by removing stopwords and non-alphabetic tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.is_alpha and not token.is_stop:\n",
    "        print(token.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is feeling more \"Seinfeld\", but it's not easy to see. Let's turn it into a pretty wordcloud! ‚òÅÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "token_texts = [t.text for t in doc]\n",
    "big_string = ' '.join(token_texts)\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(big_string) \n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very cool! But spacy also offers advanced linguistic visualizations out of the box. For example, we can show the dependency parse tree for the 42nd line of the dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(lines[42])\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use `spacy` to annotate [entities](https://en.wikipedia.org/wiki/Named-entity_recognition) in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(''.join(lines[120:130]))\n",
    "displacy.render(doc2, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So cool! Notice how `New York` is correctly tagged as a geo political entity, but George's laugh `ho` is erroneously labeled as such... This is because advanced NLP processing requires _models_. By definition, they are not perfect. This is important to note, because errors might affect downstream tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Load and Display Images\n",
    "How to Convert Images to NumPy Arrays and Back\n",
    "How to Resize Images\n",
    "How to Flip, Rotate, and Crop Images\n",
    "How to Save Images to File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's best, ice cream üç¶, or waffles üßá? \n",
    "\n",
    "We have a dataset of 20 ice cream & 20 waffles images to figure it out. We choose the [pillow](https://python-pillow.org/) library to help us in this delicious venture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "image = Image.open('waffles_or_ice_cream/ice_cream/11.jpg')\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing an image is as simple as that! We are using pillow to read the `.jpg` file, and matplotlib to view it.  \n",
    "Let's see a waffle next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('waffles_or_ice_cream/waffles/9.jpg')\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is distractingly appetizing, but can you notice the axes? They are numerically labeled. That's because images are just arrays! We can see it first hand by printing `image.size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know another library that is particularly good at handling array data, NumPy. Let's convert our pillow [`Image`](https://pillow.readthedocs.io/en/stable/handbook/concepts.html) to an `ndarray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.asarray(image)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `3`? This is a 3D array! This is because `RGB` images store 3 numerical values per pixel, one for each color. This might \"feel\" weird, since NumPy was originally designed for efficient linear algebra. But numbers is data, and images is matrices! In fact, we can display images directly using `ndarray`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can manipulate images with our knowledge of NumPy indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flip = data[::-1, :, :]\n",
    "plt.imshow(data_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flip_color = data[:, :, ::-1]\n",
    "plt.imshow(data_flip_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zoom = data[100:200, 100:200, :]\n",
    "plt.imshow(data_zoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That blue waffle doesn't look quite as appetizing. üôÖ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "pillow also offers convenient methods to carry out these common transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_flip = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "plt.imshow(image_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_crop = image.crop((100, 100, 200, 200))\n",
    "plt.imshow(image_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resize = image.resize((500,200))\n",
    "plt.imshow(image_resize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W I D E    W A F F L E ü§§\n",
    "\n",
    "These operations might seem trivial, but they are important for Machine Learning. Datasets must be cleaned and normalised to be used for training. Also, a popular way to improve model accuracy in the field of [computer vision](https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e) is to use [data augmentation](https://bair.berkeley.edu/blog/2019/06/07/data_aug/). To augment image data, we commonly have to flip, rotate, fuzz, or change pixel values.\n",
    "\n",
    "Let's save our wide waffle masterpiece:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.save('waffles_or_ice_cream/waffles/wide_waffle.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our waffles and ice creams image sizes are all over the place. Let's normalise the dataset by rescaling all of our images. \n",
    "\n",
    "üí™ Create a new dataset, `waffles_or_ice_cream_norm` which contains all the `waffles_or_ice_cream` images resized to $100x100$ pixels. Go through the lecture 1.4 notebook if you need a refresher on data pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "Today, we learned about text and image processing. We cleaned a transcript from an episode of Seinfeld, first with simple string operations, and then with the spacy library. We also loaded, manipulated, and saved images with pillow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "## Core Resources\n",
    "\n",
    "- [Text processing in python](https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908)\n",
    "- [Pillow tutorial](https://pillow.readthedocs.io/en/3.0.x/handbook/tutorial.html)\n",
    "- [Kaggle dataset - seinfeld chronicles](https://www.kaggle.com/thec03u5/seinfeld-chronicles)\n",
    "- [Kaggle dataset - waffles or ice cream](https://www.kaggle.com/sapal6/waffles-or-icecream)\n",
    "        \n",
    "## Additional Resources\n",
    "        \n",
    "- [ Text data processing walkthrough](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html)\n",
    "- [Guide to deal with text data](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/)\n",
    "- [Introduction to Natural Language Processing in python](https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63)\n",
    "- [Load and manipulate images with pillow](https://machinelearningmastery.com/how-to-load-and-manipulate-images-for-deep-learning-in-python-with-pil-pillow/)\n",
    "- [Advanced image processing with SciPy](https://scipy-lectures.org/advanced/image_processing/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

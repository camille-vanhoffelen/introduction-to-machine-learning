{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"colab":{"name":"evaluation_pt.1.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["QlqbGctBklGs"],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pRn704_VklFl","colab_type":"text"},"source":["# Lecture 4.2: Evaluation Pt.1\n","\n","[**Lecture Slides**](https://docs.google.com/presentation/d/1Z2kUep8v6dKPlUrJh7sFBawHO0h7MpvaMbY8xrVH_9I/edit?usp=sharing)\n","\n","This lecture, we are going to evaluate sklearn, keras, and pytorch models.\n","\n","**Learning goals:**\n","- split train, validation, and test set with sklearn\n","- calculate validation metrics in keras\n","- split an image dataset\n","- add validation scores to a pytorch training loop\n","- run end to end machine learning experiments\n","- compare model quality\n","- tune a hyperparameter\n","- analyze train & val loss curves for neural network optimization"]},{"cell_type":"markdown","metadata":{"id":"oLByC0onklFm","colab_type":"text"},"source":["##  1. sklearn"]},{"cell_type":"markdown","metadata":{"id":"nSalWjEAklFn","colab_type":"text"},"source":["Let's revisit the banknote authentication dataset. We have trained many models on this data in chapter 3, but never _evaluated_ them. So this time, let's follow the checklist from the lecture slides.\n","\n","#### 1.1 ü§î define ML task\n","\n","We have already defined this in lecture 3.9. We are trying to solve a binary classification task: fake vs genuine banknotes.\n","\n","#### 1.2 üìÇ find data you want to do well on\n","\n","The banknote authentication dataset is a good representation of the bills we might encounter in the \"wild\". Let's download it from amazon S3:\n"]},{"cell_type":"code","metadata":{"id":"zoZAmmMgklFo","colab_type":"code","colab":{}},"source":["!wget --quiet https://introduction-to-machine-learning-ilia-university.s3.eu-west-2.amazonaws.com/banknote.csv"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JjGlD2r1klFr","colab_type":"text"},"source":["We can then load it into a `DataFrame`:"]},{"cell_type":"code","metadata":{"id":"R8lX5suTklFs","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","df = pd.read_csv('banknote.csv')\n","df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J_jlhuhSklFu","colab_type":"text"},"source":["#### 1.3 ‚úÇÔ∏è split a test set and set it aside\n","\n","We usually jump straight into converting this `DataFrame` to features, which we then use to `.fit()` our model. This time however, we first split a test set.\n","\n","sklearn makes this easy with the `train_test_split` function. The [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) mentions that it can split many different inputs:\n","\n","> Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n","\n","We choose to split our `DataFrame` 80%/20%:"]},{"cell_type":"code","metadata":{"id":"Xh-570xUklFv","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","train_df, test_df = train_test_split(df, test_size=0.20, random_state=777)\n","print(f'total size: {len(df)}, train set size: {len(train_df)}, test set size: {len(test_df)}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TwNkHOaHklFx","colab_type":"text"},"source":["We choose to \"set aside\" the test set for later use. This prevents us from accidentally using data from the test set during development:"]},{"cell_type":"code","metadata":{"id":"OXmnuvZvklFy","colab_type":"code","colab":{}},"source":["train_df.to_csv('banknote_train.csv', index=False)\n","test_df.to_csv('banknote_test.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LIA7_-yeklF0","colab_type":"text"},"source":["#### 1.4 ‚úÇÔ∏è split train & validation sets\n","\n","We choose to split the validation set _lazily_ , meaning we won't save it to disk like the test set. This is fine, because validation sets _can_ be reused.  \n","i.e Our results won't be statistically compromised, if the split isn't the same for each round of experiments."]},{"cell_type":"code","metadata":{"id":"jStMAblHklF1","colab_type":"code","colab":{}},"source":["df = pd.read_csv('banknote_train.csv')\n","train_df, val_df = train_test_split(df, test_size=0.20, random_state=4242)\n","print(f'train set size: {len(train_df)}, validation set size: {len(val_df)}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x-U8R4ZYklF3","colab_type":"text"},"source":["#### 1.5 üéØ define single number metric\n","\n","We are dealing with a balanced binary classification task, and therefore choose accuracy as our single number metric. This sole number will define our model quality."]},{"cell_type":"markdown","metadata":{"id":"AtFwZ22FklF4","colab_type":"text"},"source":["#### 1.6 üîÅ train + validate until happy with losses and metric(s)\n","\n","We are now ready to experiment! Let's first create features and labels. We could use all four features, but it turns out that classification task is then too easy, and it wouldn't be interesting to compare training and validation metrics üòë.\n","\n","So instead, we'll pick features 2 & 4 to spice up the task difficulty üå∂Ô∏è"]},{"cell_type":"code","metadata":{"id":"DYMKTXIWklF4","colab_type":"code","colab":{}},"source":["def to_features(df):\n","    X = df[['feature_2', 'feature_4']].values\n","    y = df['is_fake'].values\n","    return X, y\n","\n","X_train, y_train = to_features(train_df)\n","X_val, y_val = to_features(val_df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9nwWjcvsklF8","colab_type":"text"},"source":["For our first round of experiments, we'd like to know which type of model best solves our task. We'll use three different classifiers:\n","- linear regression\n","- random forest\n","- SVM with RBF kernel\n","\n","We fit these models on the training data:"]},{"cell_type":"code","metadata":{"id":"9SwsEiRkklF8","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","rf_clf = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n","svm_clf = SVC(kernel='rbf', C=1000, random_state=0).fit(X_train, y_train)\n","lr_clf = LogisticRegression(random_state=0).fit(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hDyugZEoklF_","colab_type":"text"},"source":["We now want to calculate the _accuracy_ of our models. sklearn provides many metric functions in the [`sklearn.metrics`](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) module, including `accuracy_score()`. It compares labels and predictions, so we can use the `.predict()` method of the model api. For example, for our linear regression model:"]},{"cell_type":"code","metadata":{"id":"senfWBEIklF_","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","\n","# predict labels\n","y_predict = lr_clf.predict(X_val)\n","# compare them to true labels\n","accuracy_score(y_val, y_predict)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_IeO0NgTklGB","colab_type":"text"},"source":["69% accuracy, not bad!\n","\n","üß† What does `accuracy` represent? How does one calculate it?\n","\n","Since it is such a common usecase, sklearn makes evaluation even easier by assigning default metrics to popular tasks and model types. For _classifiers_ , the default metric is already accuracy, so we can use the `.score()` method from the model api directly. sklearn will predict labels and compare them to the true labels for us:"]},{"cell_type":"code","metadata":{"id":"JKlUjZUTklGC","colab_type":"code","colab":{}},"source":["lr_clf.score(X_val, y_val)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vDBJx2ymklGE","colab_type":"text"},"source":["Now that we know how to evaluate sklearn models, lets's compare all of our banknote classifiers:"]},{"cell_type":"code","metadata":{"id":"PW5gkD3gklGF","colab_type":"code","colab":{}},"source":["clfs = [rf_clf, svm_clf, lr_clf]\n","\n","for clf in clfs:\n","    accuracy = clf.score(X_val, y_val)\n","    print(f'classifier: {type(clf).__name__}, validation accuracy: {accuracy}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mp3iTqPSklGH","colab_type":"text"},"source":["Wow, these models are pretty good! ü§©\n","\n","üß† Why do you think the logistic regression model is considerably less accurate than the other two?"]},{"cell_type":"markdown","metadata":{"id":"HPZ7doJHklGI","colab_type":"text"},"source":["Let's carry out a second round of experiments to determine optimal SVM hyperparameter. We're particularly interested in `C` which controls regularization.\n","\n","üí™ Train 6 SVMs, then compare their training & validation accuracy.\n","- use the `C` values listed below\n","- store the training accuracies in a list called `train_accuracies`\n","- store the validation accuracies in a list called `val_accuracies`\n","- use the unit test to debug and verify your code"]},{"cell_type":"code","metadata":{"id":"GnDDLUOXklGI","colab_type":"code","colab":{}},"source":["c_values = [0.1, 1, 10, 100, 1000]\n","\n","#¬†INSERT YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhcIyDNdklGK","colab_type":"code","colab":{}},"source":["import math\n","\n","def print_results(c_values, train_accuracies, val_accuracies):\n","    for c, train_acc, val_acc in zip(c_values, train_accuracies, val_accuracies):\n","        print(f'C: {c}, train acc: {train_acc}, val acc: {val_acc}')\n","        \n","        \n","def test_svm_C_tuning():\n","    assert train_accuracies, \"Can't find train_accuracies. Did you use the correct variable name?\"\n","    assert val_accuracies, \"Can't find val_accuracies. Did you use the correct variable name?\"\n","    assert len(train_accuracies) == 5, f\"Expected 5 training accuracies, got {len(train_accuracies)}\"\n","    assert len(val_accuracies) == 5, f\"Expected 5 validation accuracies, got {len(val_accuracies)}\"\n","    print_results(c_values, train_accuracies, val_accuracies)\n","    assert math.isclose(4.221208, sum(train_accuracies), rel_tol=1e-5), \"Something is wrong with your training accuracy values\"\n","    assert math.isclose(4.431818, sum(val_accuracies), rel_tol=1e-5), \"Something is wrong with your validation accuracy values\"\n","    print('Success! üéâ')\n","    \n","test_svm_C_tuning()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ipxQOWwnklGN","colab_type":"text"},"source":["üß† What is the best value for the hyperparameter `C`?\n","\n","üß† For which value of `C` does the SVM seem to start overfitting?"]},{"cell_type":"markdown","metadata":{"id":"WKhubImyklGN","colab_type":"text"},"source":["#### 1.7 üìè evaluate model on test set to get final metric\n","\n","The SVM is our fake banknote detection model of choice. The International Monetary Fund would like guarantees about how well this model is going to perform in production. To know the expectation value of accuracy on unseen examples, we decide to use our _test set_ to measure the metric."]},{"cell_type":"code","metadata":{"id":"9hNTncEVklGN","colab_type":"code","colab":{}},"source":["test_df = pd.read_csv('banknote_test.csv')\n","X_test, y_test = to_features(test_df)\n","svm_clf.score(X_test, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9m504S0klGP","colab_type":"text"},"source":["üß†üß† The test accuracy is slightly lower than the validation accuracy.\n","- What does test accuracy < validation accuracy usually indicate?\n","- Is the difference significant in this case? \n","- How would you verify this?\n"]},{"cell_type":"markdown","metadata":{"id":"2j52g0fjklGQ","colab_type":"text"},"source":["## 2. Keras"]},{"cell_type":"markdown","metadata":{"id":"CwdoToErklGQ","colab_type":"text"},"source":["We want to try a keras neural network on the exact same task and dataset as section 1, with the same single number metric. We can therefore skip to the 6th checklist entry:\n","\n","#### 2.6 üîÅ train + validate until happy with losses and metric(s)\n","\n","Let's create a neural network. We'll use the exact same neural architecture as lecture 3.12:"]},{"cell_type":"code","metadata":{"id":"lzTMWeGzklGR","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","nn_clf = Sequential([\n","    Dense(6, activation='relu', input_dim=2),\n","    Dense(6, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iZnJTcZTklGU","colab_type":"text"},"source":["Before we train this neural network, we must add one extra argument to the model compilation stage. The model api provides a convenient [.`evaluate()`](https://keras.io/api/models/model_training_apis/#evaluate-method) method to calculate metrics. However, these metrics have to be specified during model compilation, and are added like this:"]},{"cell_type":"code","metadata":{"id":"tEXEkdh0klGU","colab_type":"code","colab":{}},"source":["nn_clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rzPFRxmdklGW","colab_type":"text"},"source":["We are now ready to train and evaluate our model:"]},{"cell_type":"code","metadata":{"id":"os8GsM6qklGW","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","\n","np.random.seed(1337)\n","tf.random.set_seed(666)\n","\n","history = nn_clf.fit(X_train, y_train, batch_size=32, epochs=200)\n","val_loss, val_accuracy = nn_clf.evaluate(X_val, y_val)\n","print(f'validation accuracy: {val_accuracy}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JpTicqHjklGY","colab_type":"text"},"source":["That's a lot of info! üëÄ\n","- adding `accuracy` as model metric told keras to calculate it for each epoch on the _train_ set\n","- note how `accuracy` is correlated to the loss, but is more interpretable\n","- the _validation_ accuracy after 100 epochs is ~ 90%\n","\n","The logging from keras highlights a key difference between neural networks and other models. With linear regression, SVMs, or random forests, the optimization procedure is _stable_. This allows automated stopping methods to determine when to stop training. For example, the sklearn [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model uses [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) to find the minimum of the convex loss surface. \n","\n","Neural networks, on the other hand, have highly non-convex loss surfaces, and the optimization is _unstable_. They have to use stochasticity and advanced optimization methods to reach global minima, typically over many epochs. As a result, it can be unclear when to stop training based on the loss alone. It would be really helpful however to keep track of the validation loss & accuracy throughout the epochs. We could then keep track of overfitting, and also just pick the checkpoint with the best validation metric (also called [early stopping](https://en.wikipedia.org/wiki/Early_stopping)).\n","\n","This is done in keras by either supplying a validation dataset to the `.fit()` method, or simply by providing a train/val split ratio (e.g 0.2), and letting keras do the heavy lifting. We'll opt for the latter option.\n","\n","To do so, we need to give the train + val data to keras... which we have available, since we decided to split the dataset lazily!"]},{"cell_type":"code","metadata":{"id":"O0dpaDN7klGZ","colab_type":"code","colab":{}},"source":["X, y = to_features(df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5vQxIWBklGb","colab_type":"text"},"source":["We can repeat the previous steps with the added `validation_split=0.2` argument to the `.fit()` method. We'll also use `verbose=0` to keep our output cell tidy:"]},{"cell_type":"code","metadata":{"id":"MfnO3vO9klGb","colab_type":"code","colab":{}},"source":["nn_clf = Sequential([\n","    Dense(6, activation='relu', input_dim=2),\n","    Dense(6, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","nn_clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","np.random.seed(1337)\n","tf.random.set_seed(666)\n","history = nn_clf.fit(X, y, batch_size=32, epochs=200, validation_split=0.2, verbose=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"14kZcoXmklGd","colab_type":"text"},"source":["We can now plot far more graphs than the last times we trained a neural network üìà\n","- training loss\n","- validation loss\n","- training accuracy\n","- validation accuracy\n","\n","Usually, train & val metrics are overlaid on the same graph. This makes it easy to compare the values and detect overfitting."]},{"cell_type":"code","metadata":{"id":"f4ntrbR_klGd","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()\n","\n","fig = plt.figure(figsize=(12,4))\n","\n","ax1 = fig.add_subplot(121)\n","ax1.plot(history.history['loss'], label='train')\n","ax1.plot(history.history['val_loss'], label='val')\n","ax1.legend()\n","ax1.set_title('Loss Curve')\n","\n","ax1 = fig.add_subplot(122)\n","ax1.plot(history.history['accuracy'], label='train')\n","ax1.plot(history.history['val_accuracy'], label='val')\n","ax1.legend()\n","ax1.set_title('Accuracy Curve');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UP91FkhCklGf","colab_type":"text"},"source":["üòØ That's some funky accuracy curve! This \"jump\" at 125 epochs is likely to correspond to the model suddenly learning some important feature. Remember that neural network training is rather unstable!\n","\n","Notice how this \"jump\" is _not_ reflected in the loss curves. This is why we made the effort of picking a single number metric which reflected model quality. It's now much clearer which epoch corresponds to the best model, and we have a better overview of the optimization process.\n","\n","üß† What do these curves suggest about overfitting?"]},{"cell_type":"markdown","metadata":{"id":"LSBEosSoklGf","colab_type":"text"},"source":["#### 2.7 üìè evaluate model on test set to get final metric\n","\n","Now that we have chosen our final model candidate, we can evaluate it on the test set to estimate its expected production accuracy:"]},{"cell_type":"code","metadata":{"id":"QtwvSTa4klGg","colab_type":"code","colab":{}},"source":["test_df = pd.read_csv('banknote_test.csv')\n","X_test, y_test = to_features(test_df)\n","nn_clf.evaluate(X_test, y_test, verbose=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ta2vcSyJklGi","colab_type":"text"},"source":["üëç nice score!\n","\n","üß† How come the test accuracy can be larger than the validation accuracy?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LXDKaGRdklGj","colab_type":"text"},"source":["## 3. pytorch\n","\n","Let's revisit the emoji classification dataset. The CNN we trained in lecture 3.15 had a good training loss curve, but we never _evaluated_ it, so it might have been inaccurate or overfit.\n","\n","#### 3.1 ü§î define ML task\n","\n","This is a multi-class image classification task: face üòå vs flag üá¨üá™ vs animal ü¶Ö emojis.\n","\n","#### 3.2 üìÇ find data you want to do well on\n","\n","The Unicode Consortium wishes the emoji classifier to work for _all_ colours schemes, so despite being in grayscale, the emoji image dataset is a good representation of the examples we might encounter in the \"wild\". Let's download it from amazon S3 and unarchive it:\n"]},{"cell_type":"code","metadata":{"id":"5ABAXlq4klGj","colab_type":"code","colab":{}},"source":["!wget --quiet https://introduction-to-machine-learning-ilia-university.s3.eu-west-2.amazonaws.com/emojis_4.2.tar.gz\n","!tar -xf emojis_4.2.tar.gz    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HzNjzc6fklGl","colab_type":"text"},"source":["We can now load the images with pillow:"]},{"cell_type":"code","metadata":{"id":"BTNNhMdgklGm","colab_type":"code","colab":{}},"source":["import glob\n","from PIL import Image\n","\n","data_dir = 'emojis4.2'\n","paths = glob.glob(data_dir + '/*/*.png')\n","# reproducible on different os\n","paths.sort()\n","Image.open(paths[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NXpXmRxPklGo","colab_type":"text"},"source":["#### 3.3 ‚úÇÔ∏è split a test set and set it aside\n","#### 3.4 ‚úÇÔ∏è split train & validation sets\n","\n","When dealing with images, it is often easier to eagerly split validation sets, i.e to create a dedicated `val` directory on disk. This is because images are often large files, and can't all be loaded in memory. We'll split both test and validation sets at once by reorganizing the data folders. We will place 20% of the data in a directory called `test`, and 20% of the remaining data in `val`.\n","\n","Since sklearn's `train_test_split()` function works with any iterable, we can use it to split our image paths:"]},{"cell_type":"code","metadata":{"id":"eNNPVSmmklGo","colab_type":"code","colab":{}},"source":["train_paths, test_paths = train_test_split(paths, test_size=0.20, random_state=1337)\n","train_paths, val_paths = train_test_split(train_paths, test_size=0.20, random_state=1337)\n","print(f'total size: {len(paths)}, train size: {len(train_paths)}, val size: {len(val_paths)}, test size: {len(test_paths)}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UOlV7wWfklGq","colab_type":"text"},"source":["We then create the `train`, `val`, and `test` directories. We take care to maintain their sub-directory structure, since that's our labels! (this cell can take a few minutes to copy files)"]},{"cell_type":"code","metadata":{"id":"5c09WQQTklGq","colab_type":"code","colab":{}},"source":["import os \n","\n","def create_image_folder(name, paths):\n","    !mkdir -p {name}\n","    !mkdir {name}/flags\n","    !mkdir {name}/faces\n","    !mkdir {name}/animals\n","    for path in paths:\n","        dirs = path.split('/')\n","        dirs[0] = name\n","        new_path = '/'.join(dirs)\n","        !cp {path} {new_path}\n","    return\n","\n","train_dir = 'emojis/train'\n","val_dir = 'emojis/val'\n","test_dir = 'emojis/test'\n","\n","create_image_folder('emojis/train', train_paths)\n","create_image_folder('emojis/val', val_paths)\n","create_image_folder('emojis/test', test_paths)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QlqbGctBklGs","colab_type":"text"},"source":["#### 3. 5 üéØ define single number metric\n","\n","Since this is a classification task, we'll use accuracy as our single number metric to measure model quality."]},{"cell_type":"markdown","metadata":{"id":"lRbVLj6NklGt","colab_type":"text"},"source":["#### 3.6 üîÅ train + validate until happy with losses and metric(s)\n","\n","We are ready to launch some experiments! We'll use the same `ImageFolder` dataset as lecture 3.16 to load our images. This time we'll use three preprocessing transformations:\n","- [`Grayscale`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Grayscale) to convert our arrays from 3 channels to one, since `ImageFolder` loads in RGB by default\n","- `ToTensor` to convert `ndarray`s to `Tensor`s\n","- `Normalize` to feature scale the pixels\n","\n","I already calculated the pixel value mean (0.7571) and standard deviation (0.2738), as described in this [thread](https://discuss.pytorch.org/t/computing-the-mean-and-std-of-dataset/34949/2)."]},{"cell_type":"code","metadata":{"id":"jG0DfqsuklGt","colab_type":"code","colab":{}},"source":["from torchvision import transforms\n","from torchvision import datasets\n","\n","preprocess = transforms.Compose([\n","    transforms.Grayscale(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.7571], [0.2738])\n","    ])\n","\n","train_ds = datasets.ImageFolder(train_dir, preprocess)\n","val_ds = datasets.ImageFolder(val_dir, preprocess)\n","train_ds"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_VQZfrkklGv","colab_type":"text"},"source":["The preprocessors have turned our images into $1\\times64\\times64$ tensors:"]},{"cell_type":"code","metadata":{"id":"FcRir_P_klGv","colab_type":"code","colab":{}},"source":["next(iter(train_ds))[0].shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oERpTao6klGx","colab_type":"text"},"source":["Which means we read to load our `Dataset`s into `DataLoader`s:"]},{"cell_type":"code","metadata":{"id":"jJLWXHR4klGx","colab_type":"code","colab":{}},"source":["from torch.utils.data import DataLoader\n","batch_size = 32\n","train_loader = DataLoader(dataset=train_ds, \n","                          batch_size=batch_size, \n","                          shuffle=True)\n","val_loader = DataLoader(dataset=val_ds, \n","                          batch_size=batch_size, \n","                          shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ApDJXAavklGz","colab_type":"text"},"source":["Note that this time, we built two data loaders, one for the train set, one for the validation set.\n","\n","We'll define the exact same convolutional neural network as lecture 3.15:"]},{"cell_type":"code","metadata":{"id":"Xwib9aLEklGz","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn.functional as F\n","\n","\n","class ConvNet(torch.nn.Module):\n","\n","    def __init__(self, verbose=False):\n","        super(ConvNet, self).__init__()\n","  \n","        self.verbose = verbose\n","        # 1x64x64 => 8x64x64\n","        self.conv_1 = torch.nn.Conv2d(in_channels=1,\n","                                      out_channels=8,\n","                                      kernel_size=(3, 3),\n","                                      stride=(1, 1),\n","                                      padding=1)\n","        # 8x64x64 => 8x32x32\n","        self.pool_1 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n","                                         stride=(2, 2),\n","                                         padding=0)\n","        # 8x32x32 => 16x32x32\n","        self.conv_2 = torch.nn.Conv2d(in_channels=8,\n","                                      out_channels=16,\n","                                      kernel_size=(3, 3),\n","                                      stride=(1, 1),\n","                                      padding=1)\n","        # 16x32x32 => 16x16x16                             \n","        self.pool_2 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n","                                         stride=(2, 2),\n","                                         padding=0)\n","        \n","        # 16x16x16 => 32x16x16\n","        self.conv_3 = torch.nn.Conv2d(in_channels=16,\n","                                      out_channels=32,\n","                                      kernel_size=(3, 3),\n","                                      stride=(1, 1),\n","                                      padding=1)\n","        \n","        # 16x16x32 => 8x8x32                             \n","        self.pool_3 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n","                                         stride=(2, 2),\n","                                         padding=0)\n","        \n","        # 2048 => 64\n","        self.linear_1 = torch.nn.Linear(8*8*32, 64)\n","        # 64 => 3\n","        self.linear_2 = torch.nn.Linear(64, 3)\n","\n","        \n","        \n","    def forward(self, x):\n","      x = F.relu(self.conv_1(x))\n","      x = self.pool_1(x)\n","\n","      x = F.relu(self.conv_2(x))\n","      x = self.pool_2(x)\n","\n","      x = F.relu(self.conv_3(x))\n","      x = self.pool_3(x)\n","      \n","      # flatten\n","      x = x.view(-1, 8*8*32)\n","\n","      x = F.relu(self.linear_1(x))\n","      \n","      logits = self.linear_2(x)\n","      return logits\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X23mS_9iklG1","colab_type":"text"},"source":["Last preparation touch, we initialize the torch `device`, send the model weights to it, and create our `optimizer` and `criterion`:"]},{"cell_type":"code","metadata":{"id":"8D0Jf0ZjklG2","colab_type":"code","colab":{}},"source":["import torch\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","net = ConvNet()\n","net = net.to(device)\n","\n","optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n","criterion = torch.nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3L49RuOcklG3","colab_type":"text"},"source":["Which means we are ready for training! This is similar to our previous pytorch training loops, with two additions:\n","- the accuracy metric is calculated by calculating the correct prediction ratio with `running_corrects`\n","- the validation loss and metrics are measured after training in the `# VALIDATION` section"]},{"cell_type":"code","metadata":{"id":"I9LvR1sDklG4","colab_type":"code","colab":{}},"source":["import time \n","    \n","torch.manual_seed(1337)\n","np.random.seed(666)\n","\n","start_time = time.time()    \n","\n","losses = []\n","val_losses = []\n","accuracies = []\n","val_accuracies = []\n","\n","for epoch in range(30):\n","\n","    # TRAINING\n","    net = net.train()\n","    running_losses = []\n","    running_corrects = 0\n","    for data in train_loader:\n","        \n","        # prediction\n","        features, labels = data\n","        features = features.float().to(device)\n","        labels = labels.long().to(device)\n","        logits = net(features)\n","        \n","        # loss\n","        loss = criterion(logits, labels)\n","        running_losses.append(loss.item())\n","        # accuracy\n","        _, preds = torch.max(logits.data, 1)\n","        running_corrects += (preds == labels).sum().item()\n","                \n","        # gradient descent\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    \n","    # training metrics\n","    epoch_loss = np.array(running_losses).mean()\n","    losses.append(epoch_loss)\n","    epoch_accuracy = running_corrects/len(train_loader.dataset)\n","    accuracies.append(epoch_accuracy)\n","    \n","    # VALIDATION\n","    net = net.eval()\n","    running_losses = []\n","    running_corrects = 0\n","    with torch.no_grad():\n","        for data in val_loader:\n","            \n","            # prediction\n","            features, labels = data\n","            features = features.float().to(device)\n","            labels = labels.long().to(device)\n","            logits = net(features)\n","\n","            # loss\n","            loss = criterion(logits, labels)\n","            running_losses.append(loss.item())\n","            # accuracy\n","            _, preds = torch.max(logits.data, 1)\n","            running_corrects += (preds == labels).sum().item()\n","            \n","    # validation metrics\n","    val_epoch_loss = np.array(running_losses).mean()\n","    val_losses.append(val_epoch_loss)\n","    val_epoch_accuracy = running_corrects/len(val_loader.dataset)\n","    val_accuracies.append(val_epoch_accuracy)\n","    \n","    print(f'epoch: {epoch}, loss: {epoch_loss:.6f}, val loss: {val_epoch_loss:.6f}, acc: {accuracy:.4f}, val acc: {val_epoch_accuracy:.4f}')\n","            \n","\n","stop_time = time.time()\n","print(f'total training time: {stop_time - start_time}')\n","          "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HSn5CGbYklG6","colab_type":"text"},"source":["That's a lot of code! Take your time to read through it and understand each part.\n","\n","üß† What's the use of `net.train()` and `net.eval()`?\n","\n","üß† What's the use of `torch.no_grad()`?\n","\n","üß†üß† lines 31 & 62, notice that the predictions are calculated directly on the `logits` and not on the class probabilities. Why does this work? \n","\n","‚ÑπÔ∏è Notice that we're loading the validation set in batches, although we don't strictly have to. For training, batches affect the stochasticity of gradient descent step size, but for evaluation it's just cutting up calculations in smaller chunks. This is typically done with deep learning, because the datasets can be very large, and batching better leverages GPU parallelization.\n","\n","The training loop above is long and contains a lot of duplicate code. We chose to keep the operations sequential and explicit in this notebook for demonstration purposes. But usually, ML engineers will encapsulate similar operations and use design patterns to clean the training loops. They either do it themselves for their particular usecases, or use third party wrappers like pytorch [ignite](https://github.com/pytorch/ignite).\n","\n","Now that we've added validation to our training loop, we can plot loss & accuracy curves to get insights on this CNN's optimization:"]},{"cell_type":"code","metadata":{"id":"V45pWtMEklG6","colab_type":"code","colab":{}},"source":["fig = plt.figure(figsize=(12,4))\n","\n","ax1 = fig.add_subplot(121)\n","ax1.plot(losses, label='train')\n","ax1.plot(val_losses, label='val')\n","ax1.legend()\n","ax1.set_ylabel('loss')\n","ax1.set_xlabel('epoch')\n","ax1.set_title('Loss Curve')\n","\n","ax2 = fig.add_subplot(122)\n","ax2.plot(accuracies, label='train')\n","ax2.plot(val_accuracies, label='val')\n","ax2.legend()\n","ax2.set_ylabel('accuracy')\n","ax2.set_xlabel('epoch')\n","ax2.set_title('Accuracy Curve');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YZmHgOljklG8","colab_type":"text"},"source":["The model doesn't seem to overfit, since the validation loss & accuracy are close to the training values, and the loss curve doesn't exhibit the usual overfit divergence pattern. It's also clear how long the model takes to fully converge to the best accuracy.\n","\n","üß†üß† Why are the validation curves so \"quantised\"? How could we fix this?  "]},{"cell_type":"markdown","metadata":{"id":"Bazx2i8jklG9","colab_type":"text"},"source":["#### 3.7 üìè evaluate model on test set to get final metric\n","\n","Now that we've chosen model as our best candidate, we'd like to know its expected production performance. Let's test it! First we must load the test set with the same preprocessing as training (minus the data augmentation):"]},{"cell_type":"code","metadata":{"id":"_dOfbNnxklG9","colab_type":"code","colab":{}},"source":["preprocess = transforms.Compose([\n","    transforms.Grayscale(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.7571], [0.2738])\n","    ])\n","test_ds = datasets.ImageFolder(test_dir, preprocess)\n","test_loader = DataLoader(dataset=test_ds, \n","                          batch_size=batch_size,\n","                          shuffle=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XMyV2FNQklG_","colab_type":"text"},"source":["Next we use the last model checkpoint by simply reusing the `net` variable. Let's not forget to use `net.eval()` and `torch.no_grad()` before calculating the accuracy:"]},{"cell_type":"code","metadata":{"id":"1Iz148IWklHA","colab_type":"code","colab":{}},"source":["net = net.eval()\n","running_corrects = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        features, labels = data\n","        features = features.float().to(device)\n","        labels = labels.long().to(device)\n","        logits = net(features)\n","\n","        _, preds = torch.max(logits.data, 1)\n","        running_corrects += (preds == labels).sum().item()\n","            \n","test_accuracy = running_corrects / len(test_loader.dataset)\n","print(f'test accuracy: {test_accuracy}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0gDTFfF7klHC","colab_type":"text"},"source":["That's an amazing result! The Unicode Consortium will be delighted üòä.\n","\n","üß†üß† Consider the significance of this score:\n","- what does it say about the expected generalization on unseen examples?\n","- does this really mean this is an almost perfect model?\n","- what factors affect the confidence of this score?"]},{"cell_type":"markdown","metadata":{"id":"akf45BAuklHD","colab_type":"text"},"source":["## 4. Summary\n","\n","Today, we learned about **evaluation methods**. First, we noted that training loss makes for a bad model quality metric, since it cannot detect **overfitting**. We introduced the idea of a held-out **test set** to better estimate generalization properties on unseen examples. We highlighted how test sets work if they are of the **same distribution** as the data we will encounter at prediction time, and if they are **large enough**. We then described how an independent test set can still be prone to overfitting if used as part of **model development**. Since machine learning development is **experimental** & **iterative** in nature, the data scientist introduces an **information leak** between the test set and the model hyperparameters. We introduced the **validation set** as a solution. We split the responsibilities of **comparing** models, and **assessing** models, which allows engineers to both develop and measure the quality machine learning solutions. We then showed that losses weren't always interpretable values, and introducted new **metrics**, like classification accuracy or regression MSE. We underlined the importance of choosing a **single number metric** to define model quality, and speed up model development. We then synthesized all these new workflows into a **ML development checklist**, which captures the steps of typical ML engineering experiments. Finally, we applied this checklist to three ML frameworks: sklearn, keras, and pytorch. In doing so, we built viable ML solutions from scratch for banknote authentication and emoji classification tasks.\n","\n","\n","# Resources\n","\n","## Core Resources\n","\n","- [**Slides**](https://docs.google.com/presentation/d/1Z2kUep8v6dKPlUrJh7sFBawHO0h7MpvaMbY8xrVH_9I/edit?usp=sharing)  \n","- [Machine learning yearning](https://www.deeplearning.ai/machine-learning-yearning/)  \n","The Andrew Ng reference for ML engineering, including terse and practical sections about validation and test sets\n","- [sklearn on evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)  \n","Verbose official documentation on sklearn evaluation methods and apis \n","- [Train and evaluation with keras](https://www.tensorflow.org/guide/keras/train_and_evaluate)  \n","Comprehensive official guide to validation and testing in keras\n","\n","## Additional Resources\n","\n","- [Google ML crash course - accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy)  \n","Intuitive explanation of the accuracy metric and its equation\n","- [ignite](https://github.com/pytorch/ignite#why-ignite)  \n","pytorch ecosystem library with many apis to reduce evaluation boilerplate code"]},{"cell_type":"code","metadata":{"id":"KhCJoTmQklHD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
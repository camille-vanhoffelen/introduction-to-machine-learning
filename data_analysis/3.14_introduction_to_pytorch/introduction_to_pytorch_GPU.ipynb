{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"introduction_to_pytorch_GPU.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyNeTjk2n1uTD8JkwDuOm2ks"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uGkdKjL5Lfze","colab_type":"text"},"source":["# Lecture 3.14: Introduction to Pytorch\n","\n","**This notebook is meant to be run on Google Colab. Please start with the other notebook in this directory first if you are running this in jupyter.**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"onBcbH8RpX4e"},"source":["## 5. GPUs\n","\n","### 5.1 Google Colab\n","\n","Welcome to google colab! ðŸ‘‹ This is basically a cloud hosted notebook, which can have a GPU runtime for _free_ , courtesy of Google.\n","\n","First, we need to enable GPUs for the notebook:\n","\n","- navigate to Editâ†’Notebook Settings\n","- select GPU from the Hardware Accelerator drop-down\n","\n","Once that's done, let's first check that everything is indeed in place:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BaXJ_0lBpSAl","colab":{}},"source":["import torch\n","\n","torch.cuda.is_available()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j2462sOZrotX"},"source":["This means that pytorch successfully communicated with [CUDA](https://developer.nvidia.com/cuda-zone), which is Nvidia's GPU api. Thankfully we won't have to implement any of this communication ourselves ðŸ˜…. We can check the device name:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rQbB_bRnrlMf","colab":{}},"source":["torch.cuda.get_device_name(0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hK5G7V90rnWO"},"source":["To keep GPU runtimes free, Google colab doesn't guarantee which hardware is provided to our notebook. So it's good to check which kind we got!\n","\n","Next, we want to choose the GPU as pytorch device. This is done as follows:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mTQEw2acrtOL","colab":{}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_secweZir0jS"},"source":["The returned device confirms that torch is using the GPU. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YgoxZVVur4pR"},"source":["### 5.2 Using GPUs with pytorch\n","\n","Running matrix calculations on GPU is easy with pytorch. All we have to do is to _move_ the relevant tensors to the GPU `device`. When tensor memory allocation is moved to a GPU, pytorch takes care of all the low level machinery to parallelize and speed up computation.\n","\n","Before we do that however, let's reload our dataset and build our neural network here in Google colab. Since this notebook running in the cloud, its runtime doesn't have access to local files, including the `.csv` datasets in our local repository. However, these are hosted on GitHub, so we can download them to this notebook's temporary file system with [wget](https://www.gnu.org/software/wget/):"]},{"cell_type":"code","metadata":{"id":"rwnru6WFLzQw","colab_type":"code","colab":{}},"source":["!wget https://raw.githubusercontent.com/camille-vanhoffelen/introduction-to-machine-learning/master/data_analysis/lecture3.14/bank_note.csv"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZARRa7NMhZ-","colab_type":"text"},"source":["We can now load our dataset as per usual with pandas:"]},{"cell_type":"code","metadata":{"id":"zSyeiXLNL35o","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","df = pd.read_csv('bank_note.csv')\n","df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dijnaIIHDQbj"},"source":["We can then prepare the dataset, and create the neural network. This is still exactly the same code as the other 3.14 notebook:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6rEUmHd2rwfy","colab":{}},"source":["from torch.utils.data import DataLoader\n","\n","X = df[['feature_1', 'feature_2', 'feature_3', 'feature_4']].values\n","y = df['is_fake'].values\n","\n","dataset = list(zip(X, y))\n","dataset[0]\n","\n","ds_loader = DataLoader(dataset, batch_size=32, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sGtqz9skD3NT","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # create the layers\n","        self.dense1 = nn.Linear(4, 6)\n","        self.dense2 = nn.Linear(6, 6)\n","        self.dense3 = nn.Linear(6, 1)\n","\n","    def forward(self, x):\n","        # first hidden layer\n","        x = F.relu(self.dense1(x))\n","        # second hidden layer\n","        x = F.relu(self.dense2(x))\n","        # output layer\n","        x = torch.sigmoid(self.dense3(x))\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sU90yLMiEm4J"},"source":["We are ready to train our neural network. Two extra steps are needed compared to CPU training:\n","- sending the model parameters to GPU device\n","- sending the features and labels to GPU device at every gradient descent step\n","\n","Once the memory allocation of these `Tensor`s is moved to the GPU, pytorch understands which operations need to be executed there, and takes care of the rest ðŸ¤—.\n","\n","Let's initialize our neural net and send the $\\theta$s to the GPU:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xasi8qS9FJRQ","colab":{}},"source":["net = Net()\n","net.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VRxHxP80FS0N"},"source":["That's it! Now we can run our training loop, but we'll also have to use the `.to(device)` method on the `inputs` and `labels` variable each iteration:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KRkD6rK3D7-_","colab":{}},"source":["import numpy as np\n","\n","# reproducibility\n","torch.manual_seed(1337)\n","np.random.seed(666)\n","\n","# initialization\n","net = Net()\n","net.to(device)\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n","\n","losses = []\n","\n","# loop over epochs\n","for epoch in range(100):\n","    print(f'epoch {epoch} ')\n","    \n","    # loop over batches\n","    for i, data in enumerate(ds_loader):\n","        # data loading\n","        inputs, labels = data\n","        inputs = inputs.float().to(device)\n","        labels = labels.float().view(-1, 1).to(device)\n","        \n","        # prediction\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        \n","        # optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        losses.append(loss.item())\n","\n","print('finished Training')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UkPSgCefFsb8"},"source":["That wasn't much faster than our local CPU training ðŸ˜• This is because GPU acceleration for neural networks is complicated. Not all operations are parallelised in the same way, so some layer types and widths benefit more than others. Typically, the speed up becomes obvious for large computer vision and natural language processing models. But more on that in the next lectures! \n","\n","In the meantime, let's check that our neural network was properly trained. We can use matplotlib to plot the loss curve in Google colab too:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZBWv4-agFmzu","colab":{}},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","sns.set()\n","\n","\n","fig = plt.figure(dpi=100)\n","ax = fig.add_subplot(111)\n","ax.plot(losses, lw=1)\n","ax.set_xlabel('batch')\n","ax.set_ylabel('steps')\n","ax.set_title('Loss Curve');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_-Ct9fVGGT2I"},"source":["Looks great! Congratulations on training your first neural network on a GPU ðŸŽŠ. These banknote counterfeiters better behave!\n","\n","Let's move back to the other 3.14 local jupyter notebook to conclude the lecture."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JCQzwFAYGQpg","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
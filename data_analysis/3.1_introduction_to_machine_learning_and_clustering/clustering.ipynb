{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3.1: Clustering\n",
    "\n",
    "[**Lecture Slides**](https://docs.google.com/presentation/d/19huVbcPfj-okCOjXIMG34cuQdZZP1ktswuMFjbT66ZA/edit?usp=sharing)\n",
    "\n",
    "This lecture, we are going to cluster a geospatial dataset using k-Means.\n",
    "\n",
    "**Learning goals:**\n",
    "\n",
    "- Explore data using clustering\n",
    "- Implement k-means\n",
    "- Visualize geospatial data\n",
    "- Create a ridgeline graph\n",
    "\n",
    "\n",
    "We are tasked with gathering insights into to a volcano dataset üåã, but we know nothing about geology üôà. No need to panic! We know machine learning algorithms that can help us explore the patterns in this data. \n",
    "\n",
    "\n",
    "## 1. Geospatial Data\n",
    "\n",
    "Let's start our volcanic exploration by loading the dataset into pandas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "volcanos = pd.read_csv('volcanos.csv')\n",
    "volcanos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of columns, but the most interesting are `Latitude` and `Longitude`. These are geospatial coordinates, which means it's our first opportunity to visualize some awesome maps! There are many geospatial data visualization libraries in python, but for this notebook we'll use [folium](https://python-visualization.github.io/folium/). \n",
    "\n",
    "Let's focus on the coordinates by selecting the two geospatial columns from our `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = volcanos[['Latitude', 'Longitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to initialize a [`folium.Map`](https://python-visualization.github.io/folium/modules.html#folium.folium.Map). This is done with one central location. Since our coordinates span the entire globe, this doesn't matter, and we can pick our first volcano as center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "m = folium.Map(location=coords.iloc[0], tiles='Stamen Toner', zoom_start=1)\n",
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a map! üó∫ Notice how you can move and zoom interactively. Let's populate this map with our volcanos! We'll iterate through the `DataFrame` rows using [`.iterrows()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html). Then, we can create a [`Circle`](https://python-visualization.github.io/folium/modules.html#folium.vector_layers.Circle) for each volcano location. These have to be explicitly added to the map with `.add_to(m)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in coords.iterrows():\n",
    "    folium.Circle(\n",
    "    radius=10,\n",
    "    location=row,\n",
    "    color='crimson',\n",
    "    fill=True,\n",
    ").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already much easier to understand than our tabular format! Notice how volcanos aren't spread around the globe, but instead form \"clumps\" and \"lines\". We'd like to investigate this further, but we don't have a column which categorizes the data into these groups... \n",
    "\n",
    "## 2. K-Means\n",
    "\n",
    "So we're going to have to make them ourselves! This is a clustering task, for which we will use the k-Means implementation from the [sklearn](https://scikit-learn.org/) library. Remember that k-Means is a _learning_ algorithm, so there will be two steps: fitting the data, then applying the model on the data.\n",
    "\n",
    "First let's train our k-Means model. It looks like there is a dozen \"clumps\" on the map, so we'll pick a somewhat arbitrary $k=10$. Then, we'll convert our `DataFrame` to a NumPy `ndarray` using the [`.to_numpy()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html) method, and fit the model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(coords.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is trained, and ready to be used! Next, we \"predict\" the cluster allocation of points by feeding our `DataFrame` back into the `kmeans` model. \n",
    "\n",
    "‚ÑπÔ∏è This step might sound redundant, but it is important to differentiate between _training data_ and _prediction data_. In our case, they are the same, but they don't have to be! For example, if we were aliens and our dataset contained _billions_ of volcanoes, it could be more efficient to choose a random subset of the data to train the k-Means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans = kmeans.predict(coords)\n",
    "y_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`.predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.predict) method returned a vector of integers. These are the _cluster allocations_ of our volcanos. Each cluster is labeled by an integer , and each volcano is assigned a cluster. This vector stores these assignments.\n",
    "\n",
    "This allows us to visualize the clusters on our map! We use a trick to iterate through the `coords.iterrows()` and the `y_kmeans` at the same time: the python builtin function, [`zip()`](https://docs.python.org/3.3/library/functions.html#zip):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "colors = sns.color_palette('husl', n_colors=10).as_hex()\n",
    "\n",
    "m = folium.Map(location=coords.iloc[0], tiles='Stamen Toner', zoom_start=1)\n",
    "\n",
    "for (index, row), y in zip(coords.iterrows(), y_kmeans):\n",
    "    folium.Circle(\n",
    "    radius=10,\n",
    "    location=row,\n",
    "    color=colors[y],\n",
    "    fill=True,\n",
    ").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lava hot visualization üî•Notice how k-Means identified real underlying patterns in the data, and forms geospatially coherent groups. \n",
    "\n",
    "## 3. Cluster Analysis\n",
    "\n",
    "The clusters _look_ good, but let's see if they can be useful in our data exploration. First, let's append our cluster allocation data to our `DataFrame` as a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volcanos.loc[:, 'Cluster'] = y_kmeans.copy()\n",
    "volcanos.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulating one object will be easier than two! Let's investigate distributional differences between the clusters (see lecture 2.3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volcanos.groupby('Cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the clusters have different `Evelation` averages. This suggests that by identifying volcanos that were close to eachother, k-Means also grouped the data by other criteria of similarity. In lecture 2.6, we learned how averages can be misleading, and that it's preferential to visualise entire _distributions_ of datasets. Let's do this with a [ridgeline plot](https://www.data-to-viz.com/graph/ridgeline.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "df = volcanos[[\"Elevation (Meters)\", 'Cluster']]\n",
    "\n",
    "# Initialize the FacetGrid object\n",
    "pal = sns.cubehelix_palette(15, rot=-.25, light=.7)\n",
    "g = sns.FacetGrid(df, row=\"Cluster\", hue=\"Cluster\", aspect=10, height=.5, palette=pal)\n",
    "\n",
    "# Draw the densities in a few steps\n",
    "g.map(sns.kdeplot, \"Elevation (Meters)\", clip_on=False, shade=True, alpha=1, lw=1.5, bw=.2)\n",
    "g.map(sns.kdeplot, \"Elevation (Meters)\", clip_on=False, color=\"w\", lw=2, bw=.2)\n",
    "g.map(plt.axhline, y=0, lw=2, clip_on=False)\n",
    "\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0, .2, label, fontweight=\"bold\", color=color,\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "g.map(label, \"Elevation (Meters)\")\n",
    "\n",
    "# Set the subplots to overlap\n",
    "g.fig.subplots_adjust(hspace=-.25)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a more \"advanced\" [seaborn](https://seaborn.pydata.org/) plot that can be both aesthetic and informative!\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "Today was our introduction to **data analysis**. We learned how **machine learning algorithms** differ from rule-based algorithms, and how they fall into either **supervised learning** and **unsupervised learning**. Then, we explained what **clustering** is, and identified some of its major **applications**. We defined the most popular clustering method: **k-Means**, and visualized the **Expectation-Maximization** optimisation algorithm before giving it a try ourselves. It allowed us to infer structures in a **geospatial** dataset, which gave insights into the distributions of volcanos across the Globe.\n",
    "\n",
    "\n",
    "\n",
    "# Resources\n",
    "\n",
    "\n",
    "### Core Resources\n",
    "\n",
    "- [**Slides**](https://docs.google.com/presentation/d/19huVbcPfj-okCOjXIMG34cuQdZZP1ktswuMFjbT66ZA/edit?usp=sharing)\n",
    "- [Python Data Science Handbook - k-Means](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n",
    "- [k-Means applications and drawbacks](https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a)  \n",
    "Excellent blog to further your k-Means skills with silhouette analysis and the elbow method\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [k-Means clustering from the mathematicalmonk](https://youtu.be/0MQEt10e4NM)  \n",
    "Detailed but intuitive theoretical explanation of k-Means"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

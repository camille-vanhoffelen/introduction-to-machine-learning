{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3.13: Learning Better Pt. 3\n",
    "\n",
    "[**Lecture Slides**](https://docs.google.com/presentation/d/1gCJQZkepnwXhu-IUAYsWZJrD4eFJDNwuhqyhQMs1P4w/edit?usp=sharing)\n",
    "\n",
    "This lecture, we are going to experiment with batch sizes, learning rates, & optimizers in keras, in an attempt to better understand neural network optimization.\n",
    "\n",
    "**Learning goals:**\n",
    "- examine the effect of batch size on training\n",
    "- compare the effect of learning rates on training\n",
    "- contrast the choice of optimizers\n",
    "- visualize loss curves vs epochs, batches, and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "This notebook uses the keras and tensorflow deep learning libraries. If you haven't already, please follow the setup steps in notebook 3.11 to correctly install these dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Munging\n",
    "\n",
    "We'll use the same banknote authentication dataset to explore neural network optimization hyperparameters. üí∏ We load it into a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('bank_note.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features are scaled and ready to go! üèãÔ∏è‚Äç‚ôÄÔ∏èWe'll use all 4 features and put them in a feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['feature_1', 'feature_2', 'feature_3', 'feature_4']].values\n",
    "y = df['is_fake'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Size\n",
    "\n",
    "### 3.1 loss vs epochs\n",
    "Now that we understand what `batch_size` means, let's test different flavours of gradient descent on our neural network training.\n",
    "\n",
    "We'll stick to the 2 hidden layers of 6 neurons with ReLU activation from last lecture. Let's wrap the neural network creation and training in one helper function called `.train_neural_network()`. That way, we can iterate through hyperparameters quickly to compare their effects.\n",
    "\n",
    "The function takes a feature matrix, `X`, a label vector, `y`, and optimization hyperparameters. It returns the loss `history` and the training `time`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_neural_network(X, y, optimizer='adam', **kwargs):\n",
    "    # create model\n",
    "    model = Sequential([\n",
    "        Dense(6, activation='relu', input_dim=4),\n",
    "        Dense(6, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    # training reproducibility\n",
    "    np.random.seed(1337)\n",
    "    tf.random.set_seed(666)\n",
    "    \n",
    "    # train and time model\n",
    "    start = timer()\n",
    "    history = model.fit(X, y, **kwargs)\n",
    "    end = timer()\n",
    "    \n",
    "    time = end - start\n",
    "    return history, time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ÑπÔ∏è Note how `**kwargs` prevents us from explicitly listing and passing on all the `.fit()` arguments.\n",
    "\n",
    "Now we can train neural networks with different `batch_size` to compare vanilla gradient descent, stochastic gradient descent, and various sizes of mini-batch gradient descent.\n",
    "\n",
    "This is the first time that we will meet the excellent `namedtuple`: Python is flexible and fast, and therefore has a tendency to end up cluttered with hundreds of complicated nested dictionaries üêç. One solution is to create custom _classes_ to hold this data. However it's not very \"pythonic\" make hundreds of dedicated tiny classes for every single data object. Instead, we can use a `namedtuple`. It implements the `tuple` interface, and thus can be instantiated and unpacked easily. However, it also has named fields, so it is readable and safe like a real class. More details about this handy object [here](https://pymotw.com/2/collections/namedtuple.html).\n",
    "\n",
    "We want to group _settings_ and _results_ together, and this happens often in machine learning experiments. `namedtuple` offers a terse and immutable alternative to dictionaries. We therefore create two named tuples:\n",
    "- `Setting`, with fields `batch_size` and `epochs`\n",
    "- `Result`, with fields `batch_size`, `history`, and `time`\n",
    "\n",
    "We also change the `epochs` for each `batch_size` so that the training procedure doesn't take too long. Small batch sizes have more steps per epoch, and can last a while. The setting pairs chosen below aren't special in any way: they were chosen retroactively to keep the total training time under control üëÆüèª.\n",
    "\n",
    "Let's train some neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Setting = namedtuple('Setting', ['batch_size', 'epochs'])\n",
    "settings = [Setting(1, 10), \n",
    "            Setting(2, 20), \n",
    "            Setting(8, 80), \n",
    "            Setting(32, 100), \n",
    "            Setting(128, 200), \n",
    "            Setting(len(X), 1500)]\n",
    "\n",
    "Result = namedtuple('Result', ['batch_size', 'history', 'time'])\n",
    "\n",
    "results = []\n",
    "\n",
    "for s in settings:\n",
    "    history, time = train_neural_network(X, y, batch_size=s.batch_size, epochs=s.epochs)\n",
    "    results.append(Result(s.batch_size, history, time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The losses of each neural network are stored under `results`. We can iterate through the list to plot them on the same graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "colors = sns.dark_palette(\"palegreen\")\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6), dpi=120)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    ax.plot(r.history.history['loss'], label=f'batch_size={r.batch_size}', c=colors[i], lw=3, alpha=0.8)\n",
    "    \n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlim(left= -5, right=100)\n",
    "ax.legend(loc = 'upper right', ncol=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§§ That's a lot of information! Small batch sizes take fewer epochs to converge. In our case, this is mostly due to the fact they have more gradient updates per epochs, even if each step is less precise than bigger batches.\n",
    "\n",
    "üß† How many steps are in 1 epoch of vanilla gradient descent?\n",
    "\n",
    "üß† The smaller the batch size, the lower the starting loss value. Why?\n",
    "\n",
    "We learned that stochastic gradient descent has a \"bouncy\" loss üèÄ, but here it is smooth. This is because we are plotting _epochs_ on the x-axis, not individual _steps_. Let's try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 loss vs steps\n",
    "\n",
    "By default, keras saves the loss at the end of each _epoch_. If we want it for each _batch_ , we need to write a custom [callback](https://keras.io/api/callbacks/). Remember callbacks are functions used to extend training functionality.\n",
    "\n",
    "We create a `LossPerBatch` callback, which stores the neural network loss at the end of each batch. We're extending the [`Callback`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) abstract base class, meaning we only have to override the methods we're interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class LossPerBatch(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.history = {}\n",
    "        self.history['loss'] = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.history['loss'].append(logs['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the callback by passing it as an argument to `.fit()`. We can repeat the previous experiments, this time recording the loss at each gradient descent step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [Setting(1, 10), \n",
    "            Setting(2, 20), \n",
    "            Setting(8, 80), \n",
    "            Setting(32, 100), \n",
    "            Setting(128, 200), \n",
    "            Setting(len(X), 1500)]\n",
    "\n",
    "results_per_batch = []\n",
    "\n",
    "for s in settings:\n",
    "    history = LossPerBatch()\n",
    "    _, time = train_neural_network(X, y, batch_size=s.batch_size, epochs=s.epochs, callbacks=[history])\n",
    "    results_per_batch.append(Result(s.batch_size, history, time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `history` variable stored in our `result` is now the `LossPerBatch` history, so let's visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6), dpi=120)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i, r in enumerate(results_per_batch):\n",
    "    ax.plot(r.history.history['loss'], label=f'batch_size={r.batch_size}', alpha=0.6, lw=1, c=colors[i])\n",
    "    \n",
    "ax.set_xlabel('steps')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend(loc = 'upper right', ncol=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a confusing graph üòü The stochasticity of some of the loss curves (looking at you `batch_size=1` üò°) has rendered the graph illegible. Let's pick a few batch sizes to compare side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_batch_sizes(results):\n",
    "    height = len(results) * 4\n",
    "    fig = plt.figure(figsize=(8, height), dpi=120)\n",
    "    for i, r in enumerate(results):\n",
    "        r = results[i]\n",
    "        ax = fig.add_subplot(len(results), 1, i+1)\n",
    "        ax.plot(r.history.history['loss'], label=f'batch_size={r.batch_size}', alpha=0.8, lw=1, c=colors[3])    \n",
    "        ax.set_xlabel('steps')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.legend()\n",
    "    \n",
    "compare_batch_sizes([results_per_batch[0], results_per_batch[3], results_per_batch[5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The single example updates of stochastic gradient descent create a lot of noise, which seems to slow down its convergence. It struggles to stay close to the global minimum even after reaching it.\n",
    "- Vanilla gradient descent's updates are precise, but the smoothness might get it stuck in local minima for non-convex loss surfaces.\n",
    "- Mini-batch gradient descent has just enough stochasticity to be robust yet fast.\n",
    "\n",
    "The per-step efficiency of vanilla gradient descent is still the best here, and it seems to converge to the same final loss value as the other optimizers. This suggests that the loss surface isn't very \"rough\" and that bad local minima are only a minor concern for this dataset and neural architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 loss vs time\n",
    "\n",
    "We have shown which batch sizes are most efficient per epoch and per step. However, we described earlier that batch size influences the _time_ spent on each step. ‚è± Let's find out which batch size is the _fastest_ to converge. \n",
    "\n",
    "Let's assume that each epoch takes roughly the same amount of time. Since we know the _total_ training time, we can convert the _epochs_ unit to _seconds_. An easy way of converting the units of a regularly spaced vector, is rescaling the values using `np.linspace()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6), dpi=120)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    loss = r.history.history['loss']\n",
    "    x_time = np.linspace(0, r.time, len(loss))\n",
    "    ax.plot(x_time, loss, label=f'batch_size={r.batch_size}', lw=2, alpha=0.8)\n",
    "    \n",
    "ax.set_xlabel('time / s')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlim(left= -0.5, right=6)\n",
    "ax.legend(loc = 'upper right', ncol=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the fastest batch size is neither 1 or 1372. It seems that 128 examples per gradient update is the right balance between the step speed and accuracy to converge the quickest.\n",
    "\n",
    "Of course speed isn't the only factor, and one should also pick hyperparameters which find 'good' minima. In this example however, all these settings seem to converge well.\n",
    "\n",
    "‚ÑπÔ∏è Next lecture we will cover GPU & parallelisation in machine learning. This will speed up training even further and change the relationship between batch size and training time. Remember to choose the best and fastest settings for a given dataset, task, and _hardware_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Learning Rates\n",
    "\n",
    "Now that we have a feel of what batch size is best for our optimization problem, let's investigate the effects of the _learning rate_ on training. We reuse the `train_neural_network()` helper function to iterate through a range of learning rates. The learning rate must be set on an [`Optimizer`](https://keras.io/api/optimizers/), and provided as argument to the model compilation step.\n",
    "\n",
    "We use `SGD` here, because we want to showcase the effects of a constant learning rate on gradient descent. Adaptive learning rate optimizers like `adam` will change the learning rate throughout the training, and muddle our results.\n",
    "\n",
    "We modify our `Result` namedtuple to hold the `learning_rate` instead of the `batch_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1, 1, 10]\n",
    "Result = namedtuple('Result', ['learning_rate', 'history', 'time'])\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    optimizer = SGD(learning_rate=lr)\n",
    "    history, time = train_neural_network(X, y, optimizer=optimizer, batch_size=32, epochs=100)\n",
    "    results.append(Result(lr, history, time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6), dpi=120)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    ax.plot(r.history.history['loss'], label=r.learning_rate, alpha=0.8, lw=3, c=colors[i])\n",
    "    \n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend(loc = 'upper right', ncol=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resembles the graph shown in the lecture slides:\n",
    "- 0.001 is too slow\n",
    "- 10 is too high, and fails to find the loss surface minimum\n",
    "- 1 minimizes the loss the fastest  \n",
    "\n",
    "These profiles are typical: one step size hits the right balance between convergence quality and speed.\n",
    "\n",
    "‚ÑπÔ∏è Note however that these _values_ aren't very common. For more non-convex loss surfaces, learning rates $< 0.001$ are more the norm. However, just like batch size, this is a _hyperparameter_ that must be tuned to the task at hand.\n",
    "\n",
    "Keep in mind that these experiments were run with the same `batch_size` and `epochs`. Therefore the time spent per epoch is the same for all these curves (see keras logs). The steepest curve on this graph is therefore also the fastest in seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimizers\n",
    "\n",
    "In the lecture slides, we understood the principles of momentum and adaptive learning rates behind the ‚ú®**adam**‚ú® optimizer. We also explained that there _many_ other optimizers, some of which might be better suited to our learning task. Let's try _all_ of keras' optimizers and see which minimizes our loss function the best.\n",
    "\n",
    "\n",
    "We'll want to peek at the loss per epoch and per step. We therefore update our `Result` namedtuple to hold both, so we don't have to run the experiments twice. As pointed out last lecture, the `optimizer` is given to the `.compile()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = ['sgd', 'rmsprop', 'adam', 'adadelta', 'adagrad', 'adamax', 'nadam']\n",
    "Result = namedtuple('Result', ['optimizer', 'history', 'history_per_batch', 'time'])\n",
    "\n",
    "results = []\n",
    "\n",
    "for o in optimizers:\n",
    "    history_per_batch = LossPerBatch()\n",
    "    history, time = train_neural_network(X, y, optimizer=o, batch_size=32, epochs=100, callbacks=[history_per_batch])\n",
    "    results.append(Result(o, history, history_per_batch, time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6), dpi=120)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for r in results:\n",
    "    ax.plot(r.history.history['loss'], label=r.optimizer, alpha=0.8)\n",
    "    \n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend(loc = 'upper right', ncol=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a wide range of convergence speeds. \n",
    "- the constant learning rate `sgd` is far behind, not having converged by 100 epochs\n",
    "- `nadam` leads with an impressive ~ 30 epochs to convergence üèé \n",
    "- `adam` also seems to be a solid  optimizer in 3rd place\n",
    "\n",
    "We'd like to see the loss per-step, but we anticipate those curves to be noisy. So let's pick three to compare side by side:\n",
    "- regular ol' `sgd` (mini-batch gradient descent with constant learning rate) üë¥\n",
    "- the messiah, `adam` üòá\n",
    "- the prophet's nemesis, `nadam` üòà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_optimizers(results):\n",
    "    height = len(results) * 4\n",
    "    fig = plt.figure(figsize=(8, height), dpi=120)\n",
    "    for i, r in enumerate(results):\n",
    "        r = results[i]\n",
    "        ax = fig.add_subplot(len(results), 1, i+1)\n",
    "        ax.plot(r.history_per_batch.history['loss'], label=r.optimizer, alpha=0.8, lw=1, c=colors[3])    \n",
    "        ax.set_xlabel('steps')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.legend()\n",
    "    \n",
    "compare_optimizers([results[0], results[2], results[6]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`adam` might have lost this race, but we can clearly see that advanced optimizers speed up loss function minimization. üéä\n",
    "\n",
    "In fact, notice that the adaptive learning rate methods also reduce the stochasticity later in the training, meaning they also improve the _quality_ of the optimization.\n",
    "\n",
    "\n",
    "The analysis above only proves that `nadam` is the fastest optimizer per _epoch_ and per _step_. Indeed, one of these methods could take longer to_calculate, and result in a longer total training _time_. \n",
    "\n",
    "üí™üí™ Plot the loss curves of each optimizer versus training time.\n",
    "- see section 3.3 for an example\n",
    "- the graph is the unit test üôÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Is this what you expected? Does this confirm that `nadam` is the fastest optimizer for our problem?\n",
    "\n",
    "üß†üß† How can you tell that nadam is the slowest _calculation_ per gradient descent update?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Learning Rate Scheduling\n",
    "\n",
    "Learning rate schedule are most used for their _restarts_ and _warmups_. Recall that these \"bumps\" in learning rate can help the descent jump out of bad minima or escape flat areas of the loss surface. ü§∏‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Our optimization problem has shown no issues with bad local minima. There is therefore no point trying fancy learning rate schedules, as they probably wouldn't improve anything. üòî\n",
    "\n",
    "If you are interested however, learning rate schedules can be implemented in keras using the [LearningRateScheduler](https://keras.io/api/callbacks/learning_rate_scheduler/) class, or by writing a custom [callback](https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/callbacks/cyclical_learning_rate.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "Today, we learned about **advanced optimization** algorithms. First, we described the struggles of vanilla gradient descent with **non-convex** functions. We then identified the benefits of **stochasticity**, and how reducing the **batch size** can help navigate bumpy neural network losses. We recognised **mini-batch gradient descent** as the most common gradient descent flavour today, and defined an **epoch** as a complete pass through the training data. We then highlighted the importance of the **learning rate**, i.e the gradient descent step size. We saw how **learning rate scheduling** can help find a better minimum, faster. We also learned that advanced optimizers such as **adam**, use momentum and adaptive learning rates to improve optimization. We recognized that neural network training is **complicated**, and that the best methods vary with tasks and datasets. Finally, we investigated these techniques ourselves by testing optimization hyperparameter combinations on the banknote authentication dataset.\n",
    "\n",
    "### Core Resources\n",
    "\n",
    "- [**Slides**](https://docs.google.com/presentation/d/1gCJQZkepnwXhu-IUAYsWZJrD4eFJDNwuhqyhQMs1P4w/edit?usp=sharing)\n",
    "- [Ruder on better gradient descent](https://ruder.io/optimizing-gradient-descent/)  \n",
    "Classi blogpost with clear explanations of the main neural network optimizers\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Animating gradient descent](http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/)  \n",
    "Matplolib visualization of gradient descent algorithms\n",
    "- [Loss function visualization](http://www.telesens.co/loss-landscape-viz/viewer.html)  \n",
    "Interactive app to visualize common CV loss landscapes\n",
    "- [More Ruder on better gradient descent](https://ruder.io/deep-learning-optimization-2017/)  \n",
    "Follow up to the classic blog post\n",
    "- [Recent gradient descent algorithms](https://johnchenresearch.github.io/demon/)  \n",
    "Even more recent follow up about advanced optimization methods\n",
    "- [Improving the way we work with learning rates](https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b)  \n",
    "Blogpost focusing on learning rate scheduling and restarts\n",
    "- [efficient backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)  \n",
    "Classic paper going deep into backpropagation and gradient descent algorithms for neural networks\n",
    "- [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/pdf/1206.5533v2.pdf)  \n",
    "Also classic detailed paper on how to get your neural networks to actually work\n",
    "- [Loss functions tumblr](https://lossfunctions.tumblr.com/)  \n",
    "Some fun loss curves\n",
    "- [Loss landscapes](https://losslandscape.com/)  \n",
    "Beautiful renderings of neural loss landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3.10: Random Forests\n",
    "\n",
    "[**Lecture Slides**](https://docs.google.com/presentation/d/1vpWA5RgGDv69ahG3B7kh4JozRu8LIxb29GMfRLk3KjQ/edit?usp=sharing)\n",
    "\n",
    "This lecture, we are going to train and compare a decision tree and a random forest on a real dataset.\n",
    "\n",
    "**Learning goals:**\n",
    "- train a decision tree classifier\n",
    "- train a random forest classifier\n",
    "- visualize and compare the model decision boundaries\n",
    "- analyse the effect of regularization parameters `max_depth` & `min_samples_leaf`\n",
    "- train a random forest regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Let's try to improve our fake banknote detector from lecture 3.9. üïµÔ∏è‚Äç‚ôÄÔ∏è We'll use the same [banknote authentication dataset](https://archive.ics.uci.edu/ml/datasets/banknote+authentication), and try to solve the fake/genuine classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "### 2.1 Data Munging\n",
    "\n",
    "Let's load our `.csv` into a pandas `DataFrame`, and have a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('bank_note.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are dealing with 4 features, and one binary label. The features are standardized, so no further preprocessing is necessary.\n",
    "\n",
    "We can create our feature matrix, `X`, and our label vector, `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['feature_2', 'feature_4']].values\n",
    "y = df['is_fake'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize the dataset to remember the complexity of the classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(5,5), dpi=120)\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k', alpha=0.5)\n",
    "ax.set_xlabel('feature_2')\n",
    "ax.set_ylabel('feature_4')\n",
    "ax.set_title('Banknote Classification')\n",
    "handles, labels = scatter.legend_elements()\n",
    "ax.legend(handles=handles, labels=['genuine', 'fake']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is _not separable_ , and the relationship between `feature_2` and `feature_4` is _non-linear_. This should make a good challenge for our decision trees! üå≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†2.2 Training\n",
    "\n",
    "#### 2.2.1 Decision Trees\n",
    "\n",
    "We can use our favourite sklearn model api with `.fit` and `.predict()`. The class for decision tree classifiers is ... `DecisionTreeClassifier` üòè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=0)\n",
    "tree_clf = tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Can you list all the steps that sklearn had to go through to train this decision tree with the function `.fit()`? \n",
    "\n",
    "We usually try to investigate our model parameters after fitting the model. However, decision trees don't have a vector $\\theta$ or support vectors, they are _non-parametric models_. In the lecture slides, we introduced decision trees as nested if-statements. So instead, we can investigate their _decision node splits_.\n",
    "\n",
    "Let's check the _depth_ of our decision tree. Remember this is the maximal length of a decision _branch_. We can also output the total number of leaf nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This decision tree has depth {tree_clf.get_depth()}, and contains {tree_clf.get_n_leaves()} leaves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27 decision levels, that's quite a big tree we've grown here! üå≥\n",
    "\n",
    "Recall that decision trees make predictions by stepping through their decision nodes. By visualizing our tree's nodes, we can interpret its predictions. Decision trees are sometimes called white box models, because we can examine their inner workings. More on model explainability next chapter! üìô\n",
    "\n",
    "We'll use sklearn's `.plot_tree()` method to visualize the decision nodes. We must _truncate_ the tree with a `max_depth=2`, or else the visualization will be too big to fit on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig = plt.figure(dpi=200)\n",
    "ax = fig.add_subplot(111)\n",
    "plot_tree(ax=ax, decision_tree=tree_clf, filled=True, max_depth=2, feature_names=['x1', 'x2'], rounded=True, precision=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization is packed with information ü§§\n",
    "\n",
    "- The first line of each node defines its _split_. i.e The feature and the value which partitions incoming data into its children nodes.\n",
    "\n",
    "- The second line indicates the _gini impurity_ metric associated with each split on the training dataset. Remember, low gini impurity implies homogeneity and is therefore a good thing!\n",
    "\n",
    "- The third line displays the number of training examples belonging to that node.\n",
    "\n",
    "- The fourth line shows the _class split_ of this node. i.e How many genuine vs fake bills were present in this node during training. We expect this gap to get larger as we go deeper down the branches.\n",
    "\n",
    "- The node color represents the same information as the fourth line: bluer nodes contain more genuine bills, redder nodes contain more fake bills. We also expect these hues to get more pronounced closer to the leafs nodes.\n",
    "\n",
    "Note that all lines except the first tell information specific to _training_. All that is needed for prediction is the feature values of the splits, and the class attribution of each leaf node.\n",
    "\n",
    "üß† Take your time to understand this graph and how it was \"greedily\" built during training.\n",
    "\n",
    "These splits (if-statements) shape the model's decision boundary. We'd like to visualize this along with the dataset. Since we'll be visualizing a lot of classifications in 2D throughout this notebook, let's write some helper functions (code from the [sklearn documentation](https://scikit-learn.org/0.18/auto_examples/svm/plot_iris.html)).\n",
    "\n",
    "Just like last lecture, the function `.plot_classification()` plots both the dataset and the decision boundary for a given feature matrix `X`, label vector `y`, and a classifier `clf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_decision_boundary(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    plot_decision_boundary(ax, clf, xx, yy, **params)\n",
    "\n",
    "\n",
    "def plot_classification(ax, X, y, clf):\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                      cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    scatter = ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k', alpha=1.0)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_title('Bank Notes Classification')\n",
    "    handles, labels = scatter.legend_elements()\n",
    "    ax.legend(handles=handles, labels=['genuine', 'fake'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily plot our decision tree classifier's predictions üé®:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5), dpi=120)\n",
    "ax = fig.add_subplot()\n",
    "plot_classification(ax, X, y, tree_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a funky decision boundary! üò≥ It is made exclusively of vertical and horizontal lines because predictions are made from a succession of if-statements on single features. The very thin rectangular areas are typical of _overfit_ decision trees. They try to fit single data points instead of the underlying patterns of the data.\n",
    "\n",
    "In the lecture slides, we have mentioned a powerful remedy for overfit decision trees: random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Random Forests\n",
    "\n",
    "Let's train a random forest on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=0)\n",
    "forest_clf = forest_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Can you list all the steps that sklearn had to go through to train this random forest? Take your time, there a lot of things going on in that `.fit()` function!\n",
    "\n",
    "Recall that random forests are an _ensemble_ of decision trees, and we can retrieve each tree with the `.estimators_` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This random forest is an ensemble of {len(forest_clf.estimators_)} decision trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.estimators_[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have the space to visualize 100 decision tree flow chart, let's directly plot the random forest's decision boundary with our helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5), dpi=120)\n",
    "ax = fig.add_subplot()\n",
    "plot_classification(ax, X, y, forest_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is considerably different from the single decision tree. It is still only made of vertical and horizontal lines, but this time it is far more _detailed_. This is because these are the combined lines of 100 decision boundaries.\n",
    "\n",
    "Recall that these predictions are made by majority voting. For each point in this graph, all the predictions \"votes\" from the 100 decision trees are rounded up: \n",
    "- those where there are more `fake` votes than `genuine` are shown in red\n",
    "- those where there are more `genuine` than `fake` votes, are shown in blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Comparison\n",
    "\n",
    "Let's compare these two models with another type of classifier: a RBF kernel SVM (see lecture 3.9).\n",
    "\n",
    "Just like last lecture, we can write a small loop to visualize these models next to eachother. We'll then train a non-linear SVM to compare with the decision tree and the random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_classification(X, y, clfs, titles):\n",
    "    fig = plt.figure(figsize=(14, 4), dpi=100)\n",
    "    for i, clf in enumerate(clfs):\n",
    "        ax = fig.add_subplot(1, len(clfs), i+1)\n",
    "        plot_classification(ax, X, y, clf)\n",
    "        ax.set_title(titles[i])\n",
    "        \n",
    "        \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_rbf = SVC(kernel='rbf', random_state=0)\n",
    "svm_rbf = svm_rbf.fit(X, y)\n",
    "\n",
    "compare_classification(X, y, [svm_rbf, tree_clf, forest_clf], ['RBF SVM', 'Decision Tree', 'Random Forest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three models exhibit non-linear decision boundaries, but in very different ways. \n",
    "- the SVM is busy maximizing the margin shaped by its distance features, creating smooth \"blobs\" that follow the edges of the data\n",
    "- the decision tree tries its best to split the dataset with single feature thresholds, splitting the data into nested rectangles\n",
    "- the random forest averages random variations of these rectangles, creating very detailed polygons. \n",
    "\n",
    "Notice that despite acting as a regularizer for the decision tree model, the random forest is _still_ somewhat overfit to the dataset.\n",
    "\n",
    "\n",
    "üß†üß† None of these models seem to be able to correctly predict the fake examples near $[1, -1]$. Why is that?\n",
    "\n",
    "üß†üß† In your opinion, which is the algorithm most adapted to this dataset and classification task? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our models by asking them to predict a banknote in the small `genuine` cluster on the left hand side of the graphs above.  We'll use $feature\\_1 = -1; feature\\_2 = 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = np.array([-1, 0]).reshape(1, 2)\n",
    "print(f'Features: {x_predict}')\n",
    "\n",
    "svm_rbf_prediction = svm_rbf.predict(x_predict)\n",
    "print(f'RBF SVM prediction: {svm_rbf_prediction}')\n",
    "\n",
    "tree_clf_prediction = tree_clf.predict(x_predict)\n",
    "print(f'Decision Tree prediction: {tree_clf_prediction}')\n",
    "\n",
    "forest_clf_prediction = forest_clf.predict(x_predict)\n",
    "print(f'Decision Tree prediction: {forest_clf_prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three models are able to correctly predict features in this cluster thanks to their non-linear decision boundary. ü§©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analysis\n",
    "\n",
    "#### 2.4.1 Regularization: max depth\n",
    "\n",
    "We have successfully trained decision trees, and prevented them from overfitting by _ensembling_ them into a random forest. But we haven't played with a different regularization hyperparameter: `max_depth`.\n",
    "\n",
    "`max_depth` \"cuts\" branches which are too long. i.e During training, nodes deeper than `max_depth` automatically become leaf nodes. \n",
    "\n",
    "Let's directly visualize the effect of this hyperparameter on the models's classification by plotting decision boundaries for different values of `max_depth`. We'll be using the handy `**kwargs` as arguments, here's a great [blog post](https://realpython.com/python-kwargs-and-args/) if you haven't heard about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree(X, y, **kwargs):\n",
    "    tree_clf = DecisionTreeClassifier(random_state=0, **kwargs)\n",
    "    return tree_clf.fit(X, y)\n",
    "\n",
    "max_depth_values = [2, 5, 20]\n",
    "trees = [train_tree(X, y, max_depth=m) for m in max_depth_values]\n",
    "titles = [f'max_depth={max_depth}' for max_depth in max_depth_values]\n",
    "\n",
    "compare_classification(X, y, trees, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the regularizing effect of \"cutting off\" trees branches after a certain depth. `max_depth` has for effect of reducting the number of nested if-statements, and therefore limiting the number of \"angles\" in the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Regularization: number of decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important hyperparameter in the training of a random forest is the number of decision trees which form the ensemble. In sklearn, this is controlled with the `n_estimators` argument. Let's check out its effects on the combined model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forest(X, y, **kwargs):\n",
    "    clf = RandomForestClassifier(random_state=0, **kwargs)\n",
    "    return clf.fit(X, y)\n",
    "\n",
    "n_estimators_values = [3, 10, 100]\n",
    "forests = [train_forest(X, y, n_estimators=n) for n in n_estimators_values]\n",
    "titles = [f'n_estimators={n}' for n in n_estimators_values]\n",
    "\n",
    "compare_classification(X, y, forests, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bigger ensemble means less overfitting, and a more accurate combined model. However, more `n_estimators` increases the training time! Try training a random forest with 5000 decision trees and count the seconds tick away üò™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_forest(X, y, n_estimators=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Regularization: minimum samples per leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí™üí™ Investigate the effect of the `min_samples_leaf` argument on a random forest.\n",
    "- you can use the exact same code structure as the section above\n",
    "- you don't have to redefine the function `.train_forest()`, since `**kwargs` will work with any _named argument_.\n",
    "- pick a suitable range of parameter values. You can always change them and run the cell again!\n",
    "- the unit test is having nice looking graphs üôÉ\n",
    "\n",
    "\n",
    "üß† Define the effect of the `min_samples_leaf` parameter. It might help to check out the [official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß†üß† How does `min_samples_leaf` affect the model's generalization? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression\n",
    "\n",
    "As mentioned in the lecture slides, decision trees can solve regression tasks. They do so by using _variance reduction_ instead of _homogeneity metrics_ to split each node, and by assigning _numerical values_ to each leaf node.\n",
    "\n",
    "We'd like to try this out this on our \"instagram planning\" dataset, and aim to predict the `actual_minutes` spent online from the originally `planned_minutes` (see notebook 3.7 for more a more detailed exploration of this dataset).\n",
    "\n",
    "Except this time, _you_ are going to compare and analyse these regression models! \n",
    "\n",
    "üí™üí™üí™ Train, analyse a decision tree regressor & a random forest regressor on the instagram planning dataset. Some helper functions are supplied so you can focus on the machine learning bits üòé. Here's a list of the steps you should be taking to lead your analysis:\n",
    "- load the `instagram_planning.csv` dataset into a `DataFrame`\n",
    "- optionally visualize this dataset to refresh your memory\n",
    "- create a feature matrix, `X`, and a label vector, `y`\n",
    "- fit a [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) model to the data (check the official documentation for details)\n",
    "- optionally visualize the decision tree's nodes to understand its prediction logic with `.plot_tree()`\n",
    "- fit a [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) model to the data (check the official documentation for details)\n",
    "- plot and compare their decision boundaries with `.compare_regression()`, which has the exact same interface as `.compare_classification()`.\n",
    "- the unit test is having nice looking graphs üôÉ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(ax, X, y, reg):\n",
    "\n",
    "    # plot the examples\n",
    "    ax.scatter(X, y, alpha=0.6)\n",
    "\n",
    "    # create feature matrix\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    x_line = np.linspace(xmin, xmax, 30).reshape(-1, 1)\n",
    "    \n",
    "    # predict\n",
    "    y_line = reg.predict(x_line)\n",
    "\n",
    "    # plot the hypothesis\n",
    "    ax.plot(x_line, y_line, c='g', linewidth=3)\n",
    "\n",
    "    # formatting\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_xlabel('planned online time (min)')\n",
    "    ax.set_ylabel('time spent online (min)')\n",
    "    ax.set_title('Online Procrastination');\n",
    "    \n",
    "def compare_regression(X, y, regs, titles):\n",
    "    fig = plt.figure(figsize=(14, 4), dpi=100)\n",
    "    for i, reg in enumerate(regs):\n",
    "        ax = fig.add_subplot(1, len(regs), i+1)\n",
    "        plot_regression(ax, X, y, reg)\n",
    "        ax.set_title(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Take your time to think about what happens in the `.fit()` method of the `DecisionTreeRegressor`. Can you list the main similarities and differences with a decision tree classifier?\n",
    "\n",
    "\n",
    "üß† When plotting the predictions of the decision tree and random forest with `.compare_regression()`, why are they so \"bumpy\" compare to linear or polynomial regression models?\n",
    "\n",
    "üß†üß† How are the predictions of each individual decision tree combined to make the numerical prediction of the random forest?\n",
    "\n",
    "üß† What do the `.compare_regression()` plots show about random forest regressors and regularization? \n",
    "\n",
    "üí™üí™ Feel free to investigate effect of various parameters on the regression models! You can use the same code structure as for the \"analysis\" section with the decision tree & random forest classifiers above. Remember that all parameters to play with are listed in the official documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "Today we learned about a flexible supervised learning model: **random forests**. We started by defining the the **decision tree** algorithm, and describing how it makes non-linear predictions with nested if-statements. We then went through its training procedure which uses **homogeneity metrics** or **variance reduction** to optimally split decision nodes. After noting these models' tendency to **overfit**, we introduced two **regularization** procedures: changing the trees' maximum depth, and **bagging**, which was our first example of ensemble learning. We then defined random forests as an **ensemble** of decision trees created through bagging and feature bagging.\n",
    "Finally, we applied decision trees and random forest to two tasks: **classification** with the banknote authentication dataset, and **regression** with our instagram_planning dataset. We trained and visualized the models, as well as analysing the effect of various regularization parameters.\n",
    "\n",
    "# Resources\n",
    "\n",
    "## Core Resources\n",
    "\n",
    "\n",
    "- [**Slides**](https://docs.google.com/presentation/d/1vpWA5RgGDv69ahG3B7kh4JozRu8LIxb29GMfRLk3KjQ/edit?usp=sharing)  \n",
    "- [sklearn decision trees](https://scikit-learn.org/stable/modules/tree.html)  \n",
    "Official documentation about the tree package, handy breakdown of tree models in sklearn\n",
    "- [sklearn documentation - ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html)  \n",
    "Official documentation about ensemble methods in sklearn\n",
    "- [Introduction to random forests](https://victorzhou.com/blog/intro-to-random-forests/)  \n",
    "Excellent visual blogpost which explains random forests in detail\n",
    "- [Python data science handbook - random forests](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html)  \n",
    "Practical code-along post about implementation of random forests with sklearn\n",
    "\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Understanding gini impurity](https://victorzhou.com/blog/gini-impurity/)  \n",
    "Same blog from victor zhou going into the mathematics of gini impurity\n",
    "- [random forest python](https://github.com/kevin-keraudren/randomforest-python)  \n",
    "Implementation of random forest from scratch in python\n",
    "- [Siraj - coding a random forest from scratch](https://youtu.be/QHOazyP-YlM)  \n",
    "Siraj doing his thing and coding a random forest from scratch in half an hour.\n",
    "- [Introduction to boosted trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)  \n",
    "Comprehensive description of one of the most successful algorithms in data science: xgboost\n",
    "- [args and kwargs demystified](https://realpython.com/python-kwargs-and-args/)  \n",
    "blog post about \\*\\*kwargs in python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
